{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Step 51.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SK6_iK8wa4e1",
        "outputId": "bd899983-6759-4861-b9e2-7130d9cb187e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/Shareddrives/Data/밑바닥부터 시작하는 딥러닝\n"
          ]
        }
      ],
      "source": [
        "from graphviz import Source\n",
        "from IPython.display import Image\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/Shareddrives/Data/밑바닥부터 시작하는 딥러닝"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 51 - MNIST"
      ],
      "metadata": {
        "id": "N5lKKyrpa-VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dezero\n",
        "import dezero.datasets as dsets\n",
        "from dezero import DataLoader, optimizers\n",
        "from dezero.models import MLP\n",
        "import dezero.functions as F\n",
        "\n",
        "max_epoch = 5\n",
        "batch_size = 100\n",
        "hidden_size = 1000\n",
        "\n",
        "train_set = dsets.MNIST(train=True)\n",
        "test_set = dsets.MNIST(train=False)\n",
        "train_loader = DataLoader(train_set, batch_size)\n",
        "test_loader = DataLoader(test_set, batch_size, shuffle=False)\n",
        "\n",
        "model = MLP((hidden_size, 10))\n",
        "optimizer = optimizers.SGD().setup(model)\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    sum_loss, sum_acc = 0, 0\n",
        "\n",
        "    for x, t in train_loader:   # 훈련용 미니배치 데이터\n",
        "        y = model(x)\n",
        "        loss = F.softmax_cross_entropy(y, t)\n",
        "        acc = F.accuracy(y, t)\n",
        "        model.cleargrads()\n",
        "        loss.backward()\n",
        "        optimizer.update()\n",
        "        \n",
        "        sum_loss += float(loss.data) * len(t)\n",
        "        sum_acc += float(acc.data) * len(t)\n",
        "\n",
        "    avg_loss = sum_loss / len(train_set)\n",
        "    avg_acc = sum_acc / len(train_set)\n",
        "\n",
        "    print(f'epoch: {epoch+1}')\n",
        "    print(f'train loss: {avg_loss:.4f}, accuracy: {avg_acc:.4f}')\n",
        "\n",
        "    sum_loss, sum_acc = 0, 0\n",
        "    with dezero.no_grad():  # 테스트는 기울기 불필요\n",
        "        for x, t in test_loader:\n",
        "            y = model(x)\n",
        "            loss = F.softmax_cross_entropy(y, t)\n",
        "            acc = F.accuracy(y, t)\n",
        "\n",
        "            sum_loss += float(loss.data) * len(t)\n",
        "            sum_acc += float(acc.data) * len(t)\n",
        "    \n",
        "    avg_loss = sum_loss / len(test_set)\n",
        "    avg_acc = sum_acc / len(test_set)\n",
        "\n",
        "    print(f'test loss: {avg_loss:.4f}, accuracy: {avg_acc:.4f}')\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GeshQtda_4B",
        "outputId": "13f82fe2-b74c-4a91-c065-b11eb3600210"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1\n",
            "train loss: 1.9221, accuracy: 0.5570\n",
            "test loss: 1.5452, accuracy: 0.7522\n",
            "\n",
            "epoch: 2\n",
            "train loss: 1.2881, accuracy: 0.7782\n",
            "test loss: 1.0451, accuracy: 0.8304\n",
            "\n",
            "epoch: 3\n",
            "train loss: 0.9237, accuracy: 0.8245\n",
            "test loss: 0.7885, accuracy: 0.8491\n",
            "\n",
            "epoch: 4\n",
            "train loss: 0.7371, accuracy: 0.8447\n",
            "test loss: 0.6556, accuracy: 0.8577\n",
            "\n",
            "epoch: 5\n",
            "train loss: 0.6322, accuracy: 0.8559\n",
            "test loss: 0.5738, accuracy: 0.8657\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train MNIST with ReLU activate func"
      ],
      "metadata": {
        "id": "SCncOFhRgyRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dezero\n",
        "import dezero.datasets as dsets\n",
        "from dezero import DataLoader, optimizers\n",
        "from dezero.models import MLP\n",
        "import dezero.functions as F\n",
        "\n",
        "model = MLP((hidden_size, hidden_size, 10), activation=F.relu)\n",
        "optimizer = optimizers.SGD().setup(model)\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    sum_loss, sum_acc = 0, 0\n",
        "\n",
        "    for x, t in train_loader:   # 훈련용 미니배치 데이터\n",
        "        y = model(x)\n",
        "        loss = F.softmax_cross_entropy(y, t)\n",
        "        acc = F.accuracy(y, t)\n",
        "        model.cleargrads()\n",
        "        loss.backward()\n",
        "        optimizer.update()\n",
        "        \n",
        "        sum_loss += float(loss.data) * len(t)\n",
        "        sum_acc += float(acc.data) * len(t)\n",
        "\n",
        "    avg_loss = sum_loss / len(train_set)\n",
        "    avg_acc = sum_acc / len(train_set)\n",
        "\n",
        "    print(f'epoch: {epoch+1}')\n",
        "    print(f'train loss: {avg_loss:.4f}, accuracy: {avg_acc:.4f}')\n",
        "\n",
        "    sum_loss, sum_acc = 0, 0\n",
        "    with dezero.no_grad():  # 테스트는 기울기 불필요\n",
        "        for x, t in test_loader:\n",
        "            y = model(x)\n",
        "            loss = F.softmax_cross_entropy(y, t)\n",
        "            acc = F.accuracy(y, t)\n",
        "\n",
        "            sum_loss += float(loss.data) * len(t)\n",
        "            sum_acc += float(acc.data) * len(t)\n",
        "    \n",
        "    avg_loss = sum_loss / len(test_set)\n",
        "    avg_acc = sum_acc / len(test_set)\n",
        "\n",
        "    print(f'test loss: {avg_loss:.4f}, accuracy: {avg_acc:.4f}')\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv6Ke43VgyHh",
        "outputId": "9bdafa26-325c-44d0-e2c1-2a1172503fa9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1\n",
            "train loss: 1.0524, accuracy: 0.7741\n",
            "test loss: 0.4766, accuracy: 0.8842\n",
            "\n",
            "epoch: 2\n",
            "train loss: 0.4155, accuracy: 0.8901\n",
            "test loss: 0.3467, accuracy: 0.9063\n",
            "\n",
            "epoch: 3\n",
            "train loss: 0.3364, accuracy: 0.9068\n",
            "test loss: 0.3042, accuracy: 0.9177\n",
            "\n",
            "epoch: 4\n",
            "train loss: 0.2996, accuracy: 0.9153\n",
            "test loss: 0.2748, accuracy: 0.9247\n",
            "\n",
            "epoch: 5\n",
            "train loss: 0.2750, accuracy: 0.9224\n",
            "test loss: 0.2587, accuracy: 0.9288\n",
            "\n"
          ]
        }
      ]
    }
  ]
}